{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics, utils\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(ids, cols_to_remove):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "\n",
    "    for id in ids:\n",
    "        # print(id)\n",
    "        with open(\"data/single_files/\" + id + \".pos\") as p:\n",
    "            for row in csv.reader(p, delimiter=\"\\t\"):\n",
    "                row = [i for j, i in enumerate(row) if j not in cols_to_remove]\n",
    "                row = [float(i) for i in row]\n",
    "                # print(row)\n",
    "                positives.append(row)\n",
    "        with open(\"data/single_files/\" + id + \".neg\") as n:\n",
    "            for row in csv.reader(n, delimiter=\"\\t\"):\n",
    "                row = [i for j, i in enumerate(row) if j not in cols_to_remove]\n",
    "                row = [float(i) for i in row]\n",
    "                negatives.append(row)\n",
    "\n",
    "    # print(positives)\n",
    "\n",
    "    data = positives + negatives\n",
    "    labels = [1] * len(positives) + [0] * len(negatives)\n",
    "\n",
    "    # print(data)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def calculate_performance(classifier, x_test, y_test):\n",
    "    prec = metrics.precision_score(y_test, classifier.predict(x_test), average=None)\n",
    "    sensi = metrics.recall_score(y_test, classifier.predict(x_test), average=None)\n",
    "    f1 = metrics.f1_score(y_test, classifier.predict(x_test), average=None)\n",
    "    acc = metrics.accuracy_score(y_test, classifier.predict(x_test))\n",
    "\n",
    "    performance = [round(prec[0], 3), round(prec[1], 3), round(sensi[0], 3), round(sensi[1], 3), round(f1[0], 3),\n",
    "                   round(f1[1], 3), round(acc, 3)]\n",
    "    return performance\n",
    "\n",
    "\n",
    "def bootstrapping(classifier, x_test, y_test):\n",
    "    size = int(round(len(x_test) / 2))\n",
    "    performances = []\n",
    "\n",
    "    for i in range(0, 999):\n",
    "        part_x, part_y = utils.resample(x_test, y_test, replace=True, n_samples=size)\n",
    "        performance = calculate_performance(classifier, part_x, part_y)\n",
    "        # string_performance = \"\\t\".join([str(i) for i in tmp_performance])\n",
    "        performances.append(performance)\n",
    "\n",
    "    return performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate id lists for train, cross-train and test splits\n",
    "\n",
    "ids_dna = []\n",
    "ids_enzyme = []\n",
    "\n",
    "with open(\"data/ids_dna.txt\") as f:\n",
    "    for row in f:\n",
    "        ids_dna.append(row.strip())\n",
    "\n",
    "with open(\"data/ids_enzyme.txt\") as f:\n",
    "    for row in f:\n",
    "        ids_enzyme.append(row.strip())\n",
    "        \n",
    "random_indices_dna = random.sample(range(0,len(ids_dna)),5)\n",
    "random_indices_enzyme = random.sample(range(0,len(ids_enzyme)),36)\n",
    "\n",
    "test_dna = [ids_dna[x] for x in random_indices_dna]\n",
    "test_enzyme = [ids_enzyme[x] for x in random_indices_enzyme]\n",
    "\n",
    "ids_dna = [i for j, i in enumerate(ids_dna) if j not in random_indices_dna]\n",
    "ids_enzyme = [i for j, i in enumerate(ids_enzyme) if j not in random_indices_enzyme]\n",
    "\n",
    "ids = ids_dna + ids_enzyme\n",
    "random_indices = random.sample(range(0,len(ids)),len(ids))\n",
    "\n",
    "split1_indices = random_indices[0:74]\n",
    "split2_indices = random_indices[75:149]\n",
    "split3_indices = random_indices[150:224]\n",
    "split4_indices = random_indices[225:298]\n",
    "split5_indices = random_indices[299:372]\n",
    "\n",
    "test_ids = test_dna + test_enzyme\n",
    "split1_ids = [ids[x] for x in split1_indices]\n",
    "split2_ids = [ids[x] for x in split2_indices]\n",
    "split3_ids = [ids[x] for x in split3_indices]\n",
    "split4_ids = [ids[x] for x in split4_indices]\n",
    "split5_ids = [ids[x] for x in split5_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# import data needed for the current model\n",
    "\n",
    "cols_to_remove = [3,4,8,9,10,11,12,13,14,18,19,23,24,25,26,27,28,29,33,34,38,39,40,41]\n",
    "model = \"mm2\"\n",
    "\n",
    "x_test, y_test = import_data(test_ids, cols_to_remove)\n",
    "x_split1, y_split1 = import_data(split1_ids, cols_to_remove)\n",
    "x_split2, y_split2 = import_data(split2_ids, cols_to_remove)\n",
    "x_split3, y_split3 = import_data(split3_ids, cols_to_remove)\n",
    "x_split4, y_split4 = import_data(split4_ids, cols_to_remove)\n",
    "x_split5, y_split5 = import_data(split5_ids, cols_to_remove)\n",
    "\n",
    "print(len(x_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "# train neural network and analyse performance\n",
    "\n",
    "x_train = x_split1 + x_split2 + x_split3 + x_split4\n",
    "y_train = y_split1 + y_split2 + y_split3 + y_split4\n",
    "x_cross = x_split5\n",
    "y_cross = y_split5\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train, y_train = sm.fit_sample(x_train,y_train)\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(200,), alpha=0.0001, random_state=1)\n",
    "classifier.fit(x_train,y_train)\n",
    "\n",
    "print(classifier.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# calculate performance\n",
    "\n",
    "print(len(classifier.coefs_[0]))\n",
    "\n",
    "performances = bootstrapping(classifier, x_cross, y_cross)\n",
    "performances = np.array(performances)\n",
    "\n",
    "#print(performances)\n",
    "\n",
    "mean_performances = np.mean(performances,axis=0)\n",
    "sd_performances = np.std(performances,axis=0)\n",
    "sd_performances = sd_performances/sqrt(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/performance/performance_mean.txt\",\"a\") as f:\n",
    "    f.write(model)\n",
    "    for x in mean_performances:\n",
    "        f.write(\"\\t\"+str(x))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "with open(\"data/performance/performance_sd.txt\",\"a\") as f:\n",
    "    f.write(model)\n",
    "    for x in sd_performances:\n",
    "        f.write(\"\\t\"+str(x))\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}