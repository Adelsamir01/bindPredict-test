{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics, utils\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(ids, cols_to_remove):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "\n",
    "    for id in ids:\n",
    "        # print(id)\n",
    "        with open(\"data/single_files/\" + id + \".pos\") as p:\n",
    "            for row in csv.reader(p, delimiter=\"\\t\"):\n",
    "                row = [i for j, i in enumerate(row) if j not in cols_to_remove]\n",
    "                row = [float(i) for i in row]\n",
    "                # print(row)\n",
    "                positives.append(row)\n",
    "        with open(\"data/single_files/\" + id + \".neg\") as n:\n",
    "            for row in csv.reader(n, delimiter=\"\\t\"):\n",
    "                row = [i for j, i in enumerate(row) if j not in cols_to_remove]\n",
    "                row = [float(i) for i in row]\n",
    "                negatives.append(row)\n",
    "\n",
    "    # print(positives)\n",
    "\n",
    "    data = positives + negatives\n",
    "    labels = [1] * len(positives) + [0] * len(negatives)\n",
    "\n",
    "    # print(data)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def calculate_performance(classifier, x_test, y_test):\n",
    "    prec = metrics.precision_score(y_test, classifier.predict(x_test), average=None)\n",
    "    sensi = metrics.recall_score(y_test, classifier.predict(x_test), average=None)\n",
    "    f1 = metrics.f1_score(y_test, classifier.predict(x_test), average=None)\n",
    "    acc = metrics.accuracy_score(y_test, classifier.predict(x_test))\n",
    "\n",
    "    performance = [round(prec[0], 3), round(prec[1], 3), round(sensi[0], 3), round(sensi[1], 3), round(f1[0], 3),\n",
    "                   round(f1[1], 3), round(acc, 3)]\n",
    "    return performance\n",
    "\n",
    "\n",
    "def bootstrapping(classifier, x_test, y_test):\n",
    "    size = int(round(len(x_test) / 2))\n",
    "    performances = []\n",
    "\n",
    "    for i in range(0, 999):\n",
    "        part_x, part_y = utils.resample(x_test, y_test, replace=True, n_samples=size)\n",
    "        performance = calculate_performance(classifier, part_x, part_y)\n",
    "        # string_performance = \"\\t\".join([str(i) for i in tmp_performance])\n",
    "        performances.append(performance)\n",
    "\n",
    "    return performances\n",
    "\n",
    "\n",
    "def get_average_model(data,ranges):\n",
    "    new_data = []\n",
    "        \n",
    "    for i in range(0,len(data)):\n",
    "        row = data[i]\n",
    "        new_row = []\n",
    "        for j in range(0,len(ranges)):\n",
    "            curr_range = ranges[j]\n",
    "            first = curr_range[0]\n",
    "            last = curr_range[1]+1\n",
    "            avg = sum(row[first:last])/len(row[first:last])\n",
    "            new_row.append(avg)\n",
    "        new_data.append(new_row)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate id lists for train, cross-train and test splits\n",
    "\n",
    "ids_dna = []\n",
    "ids_enzyme = []\n",
    "ids_more_data = []\n",
    "\n",
    "with open(\"data/ids_dna.txt\") as f:\n",
    "    for row in f:\n",
    "        ids_dna.append(row.strip())\n",
    "\n",
    "with open(\"data/ids_enzyme.txt\") as f:\n",
    "    for row in f:\n",
    "        ids_enzyme.append(row.strip())\n",
    "\n",
    "with open(\"data/ids_more_data.txt\") as f:\n",
    "    for row in f:\n",
    "        ids_more_data.append(row.strip())\n",
    "        \n",
    "random_indices_dna = random.sample(range(0,len(ids_dna)),5)\n",
    "random_indices_enzyme = random.sample(range(0,len(ids_enzyme)),36)\n",
    "\n",
    "test_dna = [ids_dna[x] for x in random_indices_dna]\n",
    "test_enzyme = [ids_enzyme[x] for x in random_indices_enzyme]\n",
    "\n",
    "ids_dna = [i for j, i in enumerate(ids_dna) if j not in random_indices_dna]\n",
    "ids_enzyme = [i for j, i in enumerate(ids_enzyme) if j not in random_indices_enzyme]\n",
    "\n",
    "ids = ids_dna + ids_enzyme\n",
    "random_indices = random.sample(range(0,len(ids)),len(ids))\n",
    "\n",
    "split1_indices = random_indices[0:74]\n",
    "split2_indices = random_indices[75:149]\n",
    "split3_indices = random_indices[150:224]\n",
    "split4_indices = random_indices[225:298]\n",
    "split5_indices = random_indices[299:372]\n",
    "\n",
    "test_ids = test_dna + test_enzyme\n",
    "split1_ids = [ids[x] for x in split1_indices]\n",
    "split2_ids = [ids[x] for x in split2_indices]\n",
    "split3_ids = [ids[x] for x in split3_indices]\n",
    "split4_ids = [ids[x] for x in split4_indices]\n",
    "split5_ids = [ids[x] for x in split5_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_set_ids.txt\",\"w\") as f:\n",
    "    f.write(\"\\n\".join(test_ids))\n",
    "\n",
    "with open(\"data/training_set_ids.txt\",\"w\") as f:\n",
    "    f.write(\"\\n\".join(split1_ids))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\".join(split2_ids))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\".join(split3_ids))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\".join(split4_ids))\n",
    "\n",
    "with open(\"data/cross_train_set_ids.txt\",\"w\") as f:\n",
    "    f.write(\"\\n\".join(split5_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# import data needed for the current model\n",
    "\n",
    "cols_to_remove = [40, 41]\n",
    "# ranges = [[0,0], [1,2], [3,4], [5,5], [6,7], [8,9], [10,10], [11,12], [13,14], [15,15], [16,17], [18,19], \n",
    "#          [20,20], [21,22], [23,24], [25,25], [26,27], [28,29], [30,30], [31,32], [33,34], [35,35], [36,37], [38,39]]\n",
    "\n",
    "model = \"mm3_5\"\n",
    "\n",
    "x_test, y_test = import_data(test_ids, cols_to_remove)\n",
    "x_split1, y_split1 = import_data(split1_ids, cols_to_remove)\n",
    "x_split2, y_split2 = import_data(split2_ids, cols_to_remove)\n",
    "x_split3, y_split3 = import_data(split3_ids, cols_to_remove)\n",
    "x_split4, y_split4 = import_data(split4_ids, cols_to_remove)\n",
    "x_split5, y_split5 = import_data(split5_ids, cols_to_remove)\n",
    "x_more, y_more = import_data(ids_more_data, cols_to_remove)\n",
    "\n",
    "# x_test = get_average_model(x_test,ranges)\n",
    "# x_split1 = get_average_model(x_split1,ranges)\n",
    "# x_split2 = get_average_model(x_split2,ranges)\n",
    "# x_split3 = get_average_model(x_split3,ranges)\n",
    "# x_split4 = get_average_model(x_split4,ranges)\n",
    "# x_split5 = get_average_model(x_split5,ranges)\n",
    "\n",
    "\n",
    "print(len(x_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "# train neural network and analyse performance\n",
    "\n",
    "x_train = x_split1 + x_split2 + x_split3 + x_split4 #+ x_more\n",
    "y_train = y_split1 + y_split2 + y_split3 + y_split4 #+ y_more\n",
    "x_cross = x_split5\n",
    "y_cross = y_split5\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train, y_train = sm.fit_sample(x_train,y_train)\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(200,), alpha=0.0001, random_state=1,tol=0.0000001)\n",
    "classifier.fit(x_train,y_train)\n",
    "\n",
    "print(classifier.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Python\\Python3.6\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "proba = classifier.predict_proba(x_cross)\n",
    "\n",
    "cutoffs = range(0,10000,1)\n",
    "precision = []\n",
    "coverage = []\n",
    "\n",
    "for cut in cutoffs:\n",
    "    # print(cut)\n",
    "    float_cut = cut/10000\n",
    "    \n",
    "    prediction = []\n",
    "    for el in proba:\n",
    "        if el[1]>=float_cut:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "    prec = metrics.precision_score(y_cross, prediction, average=None)[1]\n",
    "    cov = metrics.recall_score(y_cross, prediction, average=None)[1]\n",
    "    precision.append(prec)\n",
    "    coverage.append(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n0.0\n0.0\n0.9999\n"
     ]
    }
   ],
   "source": [
    "# plot figure\n",
    "\n",
    "print(classifier.classes_)\n",
    "print(prec)\n",
    "print(cov)\n",
    "print(float_cut)\n",
    "\n",
    "# print(precision)\n",
    "# print(coverage)\n",
    "\n",
    "new_coverage, new_precision = zip(*sorted(zip(coverage, precision)))\n",
    "\n",
    "# print(new_precision)\n",
    "# print(new_coverage)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.title(\"Precision-Coverage-Curve for \" + model)\n",
    "plt.xlabel(\"Coverage\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "lw = 2\n",
    "plt.plot(new_coverage, new_precision,color=\"navy\")\n",
    "plt.plot(0.33,0.2,'or',color=\"red\")\n",
    "# plt.plot(0.337,0.239,'or',color=\"green\")\n",
    "\n",
    "fig.savefig(\"D:/Dropbox/masterthesis/thesis/plots/machine_learning/prec-cov-curves/\"+model+\"_prec_cov_curve.png\")\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# calculate performance\n",
    "\n",
    "print(len(classifier.coefs_[0]))\n",
    "\n",
    "performances = bootstrapping(classifier, x_cross, y_cross)\n",
    "performances = np.array(performances)\n",
    "\n",
    "#print(performances)\n",
    "\n",
    "mean_performances = np.mean(performances,axis=0)\n",
    "sd_performances = np.std(performances,axis=0)\n",
    "sd_performances = sd_performances/sqrt(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/performance/performance_mean.txt\",\"a\") as f:\n",
    "    f.write(model)\n",
    "    for x in mean_performances:\n",
    "        f.write(\"\\t\"+str(x))\n",
    "    f.write(\"\\t\"+str(classifier.n_iter_)+\"\\t\"+str(len(classifier.coefs_[0]))+\"\\n\")\n",
    "\n",
    "with open(\"data/performance/performance_sd.txt\",\"a\") as f:\n",
    "    f.write(model)\n",
    "    for x in sd_performances:\n",
    "        f.write(\"\\t\"+str(x))\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for overtraining and different hidden units\n",
    "\n",
    "\n",
    "# hidden_layers = ((10,), (50,), (100,), (200,), (300,), (500,))\n",
    "hidden_layers = ((1,), (700,))\n",
    "iterations = (20, 40, 60, 80, 100, 120, 140, 160, 300, 500)\n",
    "\n",
    "for layers in hidden_layers:\n",
    "    print(layers)\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for iter in iterations:\n",
    "        print(iter)\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=layers,max_iter=iter,tol=-100)   \n",
    "        classifier.fit(x_train,y_train)\n",
    "        \n",
    "        train_score = classifier.score(x_train,y_train)\n",
    "        test_score = classifier.score(x_cross,y_cross)\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        \n",
    "        print(train_score)\n",
    "        print(test_score)\n",
    "        \n",
    "        print(classifier.n_iter_)\n",
    "\n",
    "    # plot figure\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plt.title(\"Validation Curve for MM3 (5) \" + str(layers))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0.6, 1.0)\n",
    "    lw = 2\n",
    "    plt.plot(iterations, train_scores, label=\"Training score\",\n",
    "             color=\"red\", lw=lw)\n",
    "    plt.plot(iterations, test_scores, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    fig.savefig(\"D:/Dropbox/masterthesis/thesis/plots/machine_learning/hidden_units/\"+model+\"_\" + str(layers) + \".png\")\n",
    "    plt.close(\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}